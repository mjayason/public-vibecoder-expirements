{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aaf8d4d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaf8d4d5",
        "outputId": "a164cdbc-b8c4-4e5b-f0eb-7c369684c9b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "bedrock = boto3.client(\n",
        "    service_name='bedrock-runtime',\n",
        "    region_name='us-east-1'  # change if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def call_claude_bedrock(prompt, model_id=\"anthropic.claude-3-sonnet-20240229\", max_tokens=4000, temperature=0.3):\n",
        "    body = {\n",
        "        \"prompt\": f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\",\n",
        "        \"max_tokens_to_sample\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
        "    }\n",
        "    response = bedrock.invoke_model(\n",
        "        modelId=model_id,\n",
        "        body=json.dumps(body),\n",
        "        contentType=\"application/json\",\n",
        "        accept=\"application/json\"\n",
        "    )\n",
        "    result = json.loads(response['body'].read())\n",
        "    return result['completion'].strip()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "USE_CLAUDE_BEDROCK = True\n",
        "\n",
        "def call_model(prompt):\n",
        "    if USE_CLAUDE_BEDROCK:\n",
        "        return call_claude_bedrock(prompt)\n",
        "    else:\n",
        "        return call_openai(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "vaqvfhjZtZUr"
      },
      "id": "vaqvfhjZtZUr",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1892d82f",
      "metadata": {
        "id": "1892d82f"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set your OpenAI API key (use environment variable or hardcode if safe)\n",
        "#openai.api_key = \"your-openai-api-key\"\n",
        "openai = OpenAI(api_key=OPENAI_API_KEY)\n",
        "model_name = \"gpt-4o\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4d4fd6e3",
      "metadata": {
        "id": "4d4fd6e3"
      },
      "outputs": [],
      "source": [
        "def call_model(prompt: str, model=model_name):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=4000,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "48eac554",
      "metadata": {
        "id": "48eac554"
      },
      "outputs": [],
      "source": [
        "\n",
        "from string import Template\n",
        "\n",
        "target_language = \"Java\"  # Options: Java, .NET, Python\n",
        "\n",
        "cot_prompt_template = Template(\"\"\"\n",
        "You are a COBOL modernization assistant.\n",
        "\n",
        "You will be given:\n",
        "1. A COBOL copybook (raw code)\n",
        "2. A JSON file generated by an ANTLR parser describing the hierarchical structure of the copybook\n",
        "\n",
        "Your task is to translate the data structure into a clean, idiomatic $target_class.\n",
        "Ensure feature parity:\n",
        "- Represent every COBOL field in $target_class format\n",
        "- Model REDEFINES, OCCURS, and nested GROUPS appropriately\n",
        "- Preserve data types, structure, and any redefined elements\n",
        "- Retain all metadata (PIC, COMP, SIGN, etc.) in comments or annotations\n",
        "- If fidelity is lost, insert a TODO or explanation inline\n",
        "\n",
        "COBOL Source:\n",
        "\\n$cobol_code\\n\n",
        "\n",
        "ANTLR JSON:\n",
        "\\n$antlr_json\\n\n",
        "\n",
        "Start by reasoning step by step and then output the full $target_class.\n",
        "\"\"\")\n",
        "\n",
        "def build_prompt(cobol_code, antlr_json, language):\n",
        "    target = {\n",
        "        \"Java\": \"Java class (POJO)\",\n",
        "        \"Python\": \"Python class (dataclass)\",\n",
        "        \".NET\": \"C# class (POCO)\"\n",
        "    }.get(language, \"Java class (POJO)\")\n",
        "    return cot_prompt_template.substitute(\n",
        "        target_class=target,\n",
        "        cobol_code=cobol_code,\n",
        "        antlr_json=antlr_json\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "01f8e010",
      "metadata": {
        "id": "01f8e010"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def normalize_name(name):\n",
        "    return name.lower().replace(\"-\", \"\").replace(\"_\", \"\")\n",
        "\n",
        "def extract_fields_from_json(json_str):\n",
        "    data = json.loads(json_str)\n",
        "    # Assuming the ANTLR JSON has a structure like {\"copybook\": {...}, \"fields\": [...]}\n",
        "    # If the structure is different, this needs to be adjusted.\n",
        "    # Based on the error, it seems the 'fields' key might be nested or named differently.\n",
        "    # Let's assume the structure is {\"file\": \"...\", \"copybook\": {\"name\": \"...\", \"fields\": [...]}}\n",
        "    # Or perhaps the fields are directly under the root like {\"fields\": [...]} which is what the original code assumed.\n",
        "    # Given the KeyError, the 'fields' key is likely nested.\n",
        "    # Let's try accessing fields under a potential 'copybook' key, or iterate through the top level keys to find a list.\n",
        "    # A common structure might be something like {\"program\": {...}, \"workingStorage\": {\"dictionary\": {...}, \"variables\": [...]}}\n",
        "    # However, the prompt mentions \"JSON file generated by an ANTLR parser describing the hierarchical structure of the copybook\".\n",
        "    # This suggests a structure more focused on the data hierarchy.\n",
        "    # Let's assume the structure is like {\"name\": \"COPYBOOK-NAME\", \"level\": 1, \"children\": [...]} where children are fields or groups.\n",
        "    # We need to recursively traverse this structure to find all fields.\n",
        "\n",
        "    def find_fields(node):\n",
        "        fields_list = []\n",
        "        if \"level\" in node and \"name\" in node:\n",
        "             # This looks like a field or group\n",
        "             field_info = {\n",
        "                \"original_name\": node[\"name\"],\n",
        "                \"normalized_name\": normalize_name(node[\"name\"]),\n",
        "                \"type\": node.get(\"picClause\"), # Assuming picClause holds the type info\n",
        "                \"redefines\": node.get(\"redefines\"),\n",
        "                \"occurs\": node.get(\"occurs\"),\n",
        "            }\n",
        "             fields_list.append(field_info)\n",
        "        if \"children\" in node:\n",
        "            for child in node[\"children\"]:\n",
        "                fields_list.extend(find_fields(child))\n",
        "        return fields_list\n",
        "\n",
        "    # Assuming the root of the JSON is the copybook structure\n",
        "    return find_fields(data)\n",
        "\n",
        "\n",
        "def extract_java_field_data(java_code_str):\n",
        "    fields = []\n",
        "    # This regex needs to be robust to different access modifiers and potential annotations\n",
        "    pattern = re.compile(r'(?:public|private|protected)?\\s+([\\w<>\\[\\]]+)\\s+(\\w+)\\s*;')\n",
        "    for match in pattern.findall(java_code_str):\n",
        "        fields.append({\n",
        "            \"normalized_name\": normalize_name(match[1]), # Use the variable name for normalization\n",
        "            \"java_type\": match[0] # Use the type\n",
        "        })\n",
        "    return fields\n",
        "\n",
        "def infer_expected_java_type(cobol_type):\n",
        "    if cobol_type is None:\n",
        "        return \"Unknown\"\n",
        "    # Improved type inference based on common COBOL PIC clauses\n",
        "    cobol_type_upper = cobol_type.upper()\n",
        "    if \"COMP-3\" in cobol_type_upper or \"V\" in cobol_type_upper: # Packed decimal or decimal with implied decimal point\n",
        "        return \"BigDecimal\"\n",
        "    elif \"COMP\" in cobol_type_upper: # Binary\n",
        "         # Need to consider size to determine int, long, etc.\n",
        "         # For simplicity, let's assume int for now, but this might need refinement\n",
        "         return \"int\"\n",
        "    elif \"X\" in cobol_type_upper: # Alphanumeric\n",
        "        return \"String\"\n",
        "    elif \"9\" in cobol_type_upper: # Numeric\n",
        "         # Need to consider size and signedness\n",
        "         # For simplicity, let's assume int for now, might need long or BigInteger\n",
        "         return \"int\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "\n",
        "def validate_parity(cobol_fields, java_fields):\n",
        "    report = []\n",
        "    java_map = {f[\"normalized_name\"]: f[\"java_type\"] for f in java_fields}\n",
        "    for f in cobol_fields:\n",
        "        norm = f[\"normalized_name\"]\n",
        "        # Check if the normalized COBOL name is a substring of any normalized Java name\n",
        "        matched = next((j_norm for j_norm in java_map.keys() if norm in j_norm), None)\n",
        "        match = matched is not None\n",
        "\n",
        "        expected = infer_expected_java_type(f[\"type\"])\n",
        "        actual = java_map.get(matched, \"\u274c\")\n",
        "        # Check if the expected type is part of the actual type string (e.g., \"List<String>\" contains \"String\")\n",
        "        type_match = \"\u2705\" if expected != \"Unknown\" and expected in actual else (\"\u26a0\ufe0f Check Needed\" if expected == \"Unknown\" else \"\u274c\")\n",
        "        # Check if OCCURS is handled, assuming List is used in Java\n",
        "        occurs_ok = \"\u2705\" if f[\"occurs\"] and \"List\" in actual else (\"\u274c\" if f[\"occurs\"] else \"N/A\")\n",
        "        redefine_note = \"\u26a0\ufe0f Manual Check Needed\" if f[\"redefines\"] else \"N/A\"\n",
        "        suggestion = \"\"\n",
        "        if not match:\n",
        "            suggestion = f\"private {expected} {norm};\" if expected != \"Unknown\" else f\"// TODO: Map COBOL field {f['original_name']} ({f['type']}) to Java\"\n",
        "        elif type_match == \"\u274c\":\n",
        "            suggestion = f\"// \u26a0\ufe0f Consider changing type to: {expected}\" if expected != \"Unknown\" else \"\"\n",
        "\n",
        "        report.append({\n",
        "            \"COBOL Field\": f[\"original_name\"],\n",
        "            \"COBOL Type\": f[\"type\"],\n",
        "            \"OCCURS\": f[\"occurs\"],\n",
        "            \"REDEFINES\": f[\"redefines\"],\n",
        "            \"Mapped Java Field\": matched if match else \"\u274c Not Found\",\n",
        "            \"Java Type\": actual,\n",
        "            \"Expected Java Type\": expected,\n",
        "            \"Type Match\": type_match,\n",
        "            \"OCCURS Used in Java\": occurs_ok,\n",
        "            \"REDEFINES Documented\": redefine_note,\n",
        "            \"Suggested Java Field\": suggestion\n",
        "        })\n",
        "    return pd.DataFrame(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fe708337",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe708337",
        "outputId": "aa37d0fe-b6f0-4909-ac27-5adb3533e2c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Processed: data \u2192 ./batch_output/data.java\n",
            "\u2705 Processed: perform_test \u2192 ./batch_output/perform_test.java\n",
            "\u2705 Processed: operations \u2192 ./batch_output/operations.java\n",
            "\u2705 Processed: file_io_test \u2192 ./batch_output/file_io_test.java\n",
            "\u2705 Processed: main \u2192 ./batch_output/main.java\n"
          ]
        }
      ],
      "source": [
        "input_dir = \"./batch_input\"\n",
        "output_dir = \"./batch_output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith(\".cob\"):\n",
        "        base = filename.replace(\".cob\", \"\")\n",
        "        cobol_path = os.path.join(input_dir, f\"{base}.cob\")\n",
        "        json_path = os.path.join(input_dir, f\"{base}.json\")\n",
        "\n",
        "        try:\n",
        "            with open(cobol_path, \"r\") as f:\n",
        "                cobol_code = f.read()\n",
        "            with open(json_path, \"r\") as f:\n",
        "                antlr_json = f.read()\n",
        "        except:\n",
        "            print(f\"Skipping {base}: missing files\")\n",
        "            continue\n",
        "\n",
        "        prompt = build_prompt(cobol_code, antlr_json, target_language)\n",
        "        model_output = call_model(prompt)\n",
        "\n",
        "        ext = {\n",
        "            \"Java\": \".java\",\n",
        "            \".NET\": \".cs\",\n",
        "            \"Python\": \".py\"\n",
        "        }.get(target_language, \".java\")\n",
        "\n",
        "        out_file = os.path.join(output_dir, f\"{base}{ext}\")\n",
        "        with open(out_file, \"w\") as f:\n",
        "            f.write(model_output)\n",
        "\n",
        "        cobol_fields = extract_fields_from_json(antlr_json)\n",
        "        java_fields = extract_java_field_data(model_output)\n",
        "        report_df = validate_parity(cobol_fields, java_fields)\n",
        "\n",
        "        report_df.to_excel(os.path.join(output_dir, f\"{base}_parity.xlsx\"), index=False)\n",
        "        report_df.to_csv(os.path.join(output_dir, f\"{base}_parity.csv\"), index=False)\n",
        "\n",
        "        print(f\"\u2705 Processed: {base} \u2192 {out_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}