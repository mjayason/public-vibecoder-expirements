{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "def invoke_bedrock(prompt, model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"):\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 2048,\n",
    "            \"temperature\": 0.7\n",
    "        })\n",
    "    )\n",
    "    result = json.loads(response['body'].read())\n",
    "    return result.get(\"completion\", result.get(\"generation\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf8d4d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaf8d4d5",
    "outputId": "a414dc8f-6d8f-42cf-ce85-77ac1eb02ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vaqvfhjZtZUr",
   "metadata": {
    "id": "vaqvfhjZtZUr"
   },
   "outputs": [],
   "source": [
    "# Used to securely store your API key\n",
    "from google.colab import userdata\n",
    "\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1892d82f",
   "metadata": {
    "id": "1892d82f"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your OpenAI API key (use environment variable or hardcode if safe)\n",
    "#openai.api_key = \"your-openai-api-key\"\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "model_name = \"gpt-4o\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d4fd6e3",
   "metadata": {
    "id": "4d4fd6e3"
   },
   "outputs": [],
   "source": [
    "def call_openai(prompt: str, model=model_name):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=4000,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dz9T6Ssd8fWR",
   "metadata": {
    "id": "dz9T6Ssd8fWR"
   },
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import yaml\n",
    "\n",
    "modernization_prompt_template = Template(\"\"\"\n",
    "You are a COBOL modernization assistant.\n",
    "\n",
    "You will be given:\n",
    "1. A COBOL source file (.cob) containing logic, I/O operations, and business rules\n",
    "2. A structured JSON file parsed by an ANTLR-based tool describing:\n",
    "   - Statement types (IF, PERFORM, CALL, DISPLAY, etc.)\n",
    "   - Line numbers and variable declarations (working-storage)\n",
    "   - Call graph and procedural structure\n",
    "3. A call graph in Markdown (.md) indicating external modules (e.g., CALL 'DATAPROGRAM')\n",
    "4. A modernization map in YAML describing how COBOL technologies (e.g., DB2, MQ, VSAM) map to modern equivalents (e.g., JDBC, Kafka, JSON)\n",
    "\n",
    "You should use this modernization mapping to guide your translation decisions:\n",
    "$modern_stack_yaml\n",
    "\n",
    "Your task is to:\n",
    "- Translate the full COBOL program into a clean, modular $target_class\n",
    "- Use the .cob file as the source of truth for string literals and logic syntax\n",
    "- Use the JSON to guide nesting, control flow, and variable declarations\n",
    "- Use the call graph to modularize external calls (e.g., create a BankAccount or DataProgram class)\n",
    "\n",
    "### Requirements:\n",
    "- Encapsulate global data like `FINAL-BALANCE` using class fields + getter/setter methods\n",
    "- Replace any external `CALL 'DATAPROGRAM' USING 'READ'` or `'WRITE'` with appropriate class methods\n",
    "  - Implement DB2-based calls using JDBC patterns (via modernization map)\n",
    "- Use idiomatic $language_code with appropriate control structures and data types\n",
    "- Add inline comments that trace back to COBOL logic where helpful\n",
    "\n",
    "### Additional Requirement:\n",
    "- Generate a unit test class (e.g., `OperationsTest.java`) using a modern test framework\n",
    "  - Use JUnit (Java), pytest (Python), or xUnit (C#)\n",
    "  - Cover edge cases like insufficient balance, credit update, and balance inquiry\n",
    "  - Use mocks or stubs for external calls (e.g., mock DB read/write)\n",
    "  - Ensure all business rules are tested in isolation\n",
    "\n",
    "### Final Output:\n",
    "- Business logic class (e.g., `Operations.java`)\n",
    "- Supporting class for persistence or external calls (e.g., `BankAccount.java`)\n",
    "- Full unit test class with coverage of key flows (e.g., `OperationsTest.java`)\n",
    "- Ensure the code is idiomatic, testable, and works in a modern $language_code environment\n",
    "- If fidelity is lost, include `// TODO` with justification\n",
    "\n",
    "COBOL Source (.cob):\n",
    "\\n$cobol_code\\n\n",
    "\n",
    "Structured JSON (.json):\n",
    "\\n$antlr_json\\n\n",
    "\n",
    "Call Graph (.md):\n",
    "\\n$callgraph_md\\n\n",
    "\n",
    "Start by reasoning step by step, then output the business logic classes and unit tests.\n",
    "\"\"\")\n",
    "\n",
    "def build_prompt(cobol_code, antlr_json, callgraph_md, language, modernization_yaml_path=\"modernization_map.yaml\"):\n",
    "    # Load modernization map\n",
    "    try:\n",
    "        with open(modernization_yaml_path, \"r\") as f:\n",
    "            modernization_map = yaml.safe_load(f)\n",
    "            modern_stack_yaml = yaml.dump(modernization_map, default_flow_style=False)\n",
    "    except Exception as e:\n",
    "        modern_stack_yaml = \"# YAML loading failed: \" + str(e)\n",
    "\n",
    "    language_map = {\n",
    "        \"Java\": \"Java application with JDBC and modular classes\",\n",
    "        \"Python\": \"Python module using classes and file I/O\",\n",
    "        \".NET\": \"C# console app with POCO and Entity Framework stubs\"\n",
    "    }\n",
    "\n",
    "    return modernization_prompt_template.substitute(\n",
    "        target_class=language_map.get(language, \"Java application with JDBC and modular classes\"),\n",
    "        modernization_stack=\"\",\n",
    "        modern_stack_yaml=modern_stack_yaml,\n",
    "        cobol_code=cobol_code,\n",
    "        antlr_json=antlr_json,\n",
    "        callgraph_md=callgraph_md,\n",
    "        language_code=language\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01f8e010",
   "metadata": {
    "id": "01f8e010"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_name(name):\n",
    "    return name.lower().replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "def extract_fields_from_json(json_str):\n",
    "    data = json.loads(json_str)\n",
    "    # Assuming the ANTLR JSON has a structure like {\"copybook\": {...}, \"fields\": [...]}\n",
    "    # If the structure is different, this needs to be adjusted.\n",
    "    # Based on the error, it seems the 'fields' key might be nested or named differently.\n",
    "    # Let's assume the structure is {\"file\": \"...\", \"copybook\": {\"name\": \"...\", \"fields\": [...]}}\n",
    "    # Or perhaps the fields are directly under the root like {\"fields\": [...]} which is what the original code assumed.\n",
    "    # Given the KeyError, the 'fields' key is likely nested.\n",
    "    # Let's try accessing fields under a potential 'copybook' key, or iterate through the top level keys to find a list.\n",
    "    # A common structure might be something like {\"program\": {...}, \"workingStorage\": {\"dictionary\": {...}, \"variables\": [...]}}\n",
    "    # However, the prompt mentions \"JSON file generated by an ANTLR parser describing the hierarchical structure of the copybook\".\n",
    "    # This suggests a structure more focused on the data hierarchy.\n",
    "    # Let's assume the structure is like {\"name\": \"COPYBOOK-NAME\", \"level\": 1, \"children\": [...]} where children are fields or groups.\n",
    "    # We need to recursively traverse this structure to find all fields.\n",
    "\n",
    "    def find_fields(node):\n",
    "        fields_list = []\n",
    "        if \"level\" in node and \"name\" in node:\n",
    "             # This looks like a field or group\n",
    "             field_info = {\n",
    "                \"original_name\": node[\"name\"],\n",
    "                \"normalized_name\": normalize_name(node[\"name\"]),\n",
    "                \"type\": node.get(\"picClause\"), # Assuming picClause holds the type info\n",
    "                \"redefines\": node.get(\"redefines\"),\n",
    "                \"occurs\": node.get(\"occurs\"),\n",
    "            }\n",
    "             fields_list.append(field_info)\n",
    "        if \"children\" in node:\n",
    "            for child in node[\"children\"]:\n",
    "                fields_list.extend(find_fields(child))\n",
    "        return fields_list\n",
    "\n",
    "    # Assuming the root of the JSON is the copybook structure\n",
    "    return find_fields(data)\n",
    "\n",
    "\n",
    "def extract_java_field_data(java_code_str):\n",
    "    fields = []\n",
    "    # This regex needs to be robust to different access modifiers and potential annotations\n",
    "    pattern = re.compile(r'(?:public|private|protected)?\\s+([\\w<>\\[\\]]+)\\s+(\\w+)\\s*;')\n",
    "    for match in pattern.findall(java_code_str):\n",
    "        fields.append({\n",
    "            \"normalized_name\": normalize_name(match[1]), # Use the variable name for normalization\n",
    "            \"java_type\": match[0] # Use the type\n",
    "        })\n",
    "    return fields\n",
    "\n",
    "def infer_expected_java_type(cobol_type):\n",
    "    if cobol_type is None:\n",
    "        return \"Unknown\"\n",
    "    # Improved type inference based on common COBOL PIC clauses\n",
    "    cobol_type_upper = cobol_type.upper()\n",
    "    if \"COMP-3\" in cobol_type_upper or \"V\" in cobol_type_upper: # Packed decimal or decimal with implied decimal point\n",
    "        return \"BigDecimal\"\n",
    "    elif \"COMP\" in cobol_type_upper: # Binary\n",
    "         # Need to consider size to determine int, long, etc.\n",
    "         # For simplicity, let's assume int for now, but this might need refinement\n",
    "         return \"int\"\n",
    "    elif \"X\" in cobol_type_upper: # Alphanumeric\n",
    "        return \"String\"\n",
    "    elif \"9\" in cobol_type_upper: # Numeric\n",
    "         # Need to consider size and signedness\n",
    "         # For simplicity, let's assume int for now, might need long or BigInteger\n",
    "         return \"int\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def validate_parity(cobol_fields, java_fields):\n",
    "    report = []\n",
    "    java_map = {f[\"normalized_name\"]: f[\"java_type\"] for f in java_fields}\n",
    "    for f in cobol_fields:\n",
    "        norm = f[\"normalized_name\"]\n",
    "        # Check if the normalized COBOL name is a substring of any normalized Java name\n",
    "        matched = next((j_norm for j_norm in java_map.keys() if norm in j_norm), None)\n",
    "        match = matched is not None\n",
    "\n",
    "        expected = infer_expected_java_type(f[\"type\"])\n",
    "        actual = java_map.get(matched, \"❌\")\n",
    "        # Check if the expected type is part of the actual type string (e.g., \"List<String>\" contains \"String\")\n",
    "        type_match = \"✅\" if expected != \"Unknown\" and expected in actual else (\"⚠️ Check Needed\" if expected == \"Unknown\" else \"❌\")\n",
    "        # Check if OCCURS is handled, assuming List is used in Java\n",
    "        occurs_ok = \"✅\" if f[\"occurs\"] and \"List\" in actual else (\"❌\" if f[\"occurs\"] else \"N/A\")\n",
    "        redefine_note = \"⚠️ Manual Check Needed\" if f[\"redefines\"] else \"N/A\"\n",
    "        suggestion = \"\"\n",
    "        if not match:\n",
    "            suggestion = f\"private {expected} {norm};\" if expected != \"Unknown\" else f\"// TODO: Map COBOL field {f['original_name']} ({f['type']}) to Java\"\n",
    "        elif type_match == \"❌\":\n",
    "            suggestion = f\"// ⚠️ Consider changing type to: {expected}\" if expected != \"Unknown\" else \"\"\n",
    "\n",
    "        report.append({\n",
    "            \"COBOL Field\": f[\"original_name\"],\n",
    "            \"COBOL Type\": f[\"type\"],\n",
    "            \"OCCURS\": f[\"occurs\"],\n",
    "            \"REDEFINES\": f[\"redefines\"],\n",
    "            \"Mapped Java Field\": matched if match else \"❌ Not Found\",\n",
    "            \"Java Type\": actual,\n",
    "            \"Expected Java Type\": expected,\n",
    "            \"Type Match\": type_match,\n",
    "            \"OCCURS Used in Java\": occurs_ok,\n",
    "            \"REDEFINES Documented\": redefine_note,\n",
    "            \"Suggested Java Field\": suggestion\n",
    "        })\n",
    "    return pd.DataFrame(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dGNnBgGs8mtX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGNnBgGs8mtX",
    "outputId": "63a268c7-1c52-4dfd-e1a2-90263814e726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: operations → ./batch_output/operations.java\n",
      "✅ Processed: perform_test → ./batch_output/perform_test.java\n",
      "✅ Processed: file_io_test → ./batch_output/file_io_test.java\n",
      "✅ Processed: main → ./batch_output/main.java\n",
      "✅ Processed: data → ./batch_output/data.java\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"./batch_input\"\n",
    "output_dir = \"./batch_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".cob\"):\n",
    "        base = filename.replace(\".cob\", \"\")\n",
    "        cobol_path = os.path.join(input_dir, f\"{base}.cob\")\n",
    "        json_path = os.path.join(input_dir, f\"{base}.json\")\n",
    "        call_graph_path = os.path.join(input_dir, f\"callgraph_{base}.md\")\n",
    "\n",
    "        try:\n",
    "            with open(cobol_path, \"r\") as f:\n",
    "                cobol_code = f.read()\n",
    "            with open(json_path, \"r\") as f:\n",
    "                antlr_json = f.read()\n",
    "            with open(call_graph_path, \"r\") as f:\n",
    "                callgraph_md = f.read()\n",
    "        except:\n",
    "            print(f\"Skipping {base}: missing files\")\n",
    "            continue\n",
    "\n",
    "        prompt = build_prompt(cobol_code, antlr_json, callgraph_md, \"Java\")\n",
    "        #print(prompt)\n",
    "\n",
    "        model_output = call_openai(prompt)\n",
    "\n",
    "        ext = {\n",
    "            \"Java\": \".java\",\n",
    "            \".NET\": \".cs\",\n",
    "            \"Python\": \".py\"\n",
    "        }.get(target_language, \".java\")\n",
    "\n",
    "        out_file = os.path.join(output_dir, f\"{base}{ext}\")\n",
    "        with open(out_file, \"w\") as f:\n",
    "            f.write(model_output)\n",
    "\n",
    "        cobol_fields = extract_fields_from_json(antlr_json)\n",
    "        java_fields = extract_java_field_data(model_output)\n",
    "        report_df = validate_parity(cobol_fields, java_fields)\n",
    "\n",
    "        report_df.to_excel(os.path.join(output_dir, f\"{base}_parity.xlsx\"), index=False)\n",
    "        report_df.to_csv(os.path.join(output_dir, f\"{base}_parity.csv\"), index=False)\n",
    "\n",
    "        print(f\"✅ Processed: {base} → {out_file}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
